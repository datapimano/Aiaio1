inv
}
# construct the sentiment score function
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr);
require(stringr);
scores = laply(sentences, function(sentence, pos.words, neg.words) {
sentence = gsub('[^A-z ]','', sentence)
sentence = tolower(sentence);
word.list = str_split(sentence, '\\s+');
words = unlist(word.list);
pos.matches = match(words, pos.words);
neg.matches = match(words, neg.words);
pos.matches = !is.na(pos.matches);
neg.matches = !is.na(neg.matches);
score = sum(pos.matches) - sum(neg.matches);
return(score);
}, pos.words, neg.words, .progress=.progress );
scores.df = data.frame(score=scores, text=sentences);
return(scores.df);
}
# load word dictionaries
positive=scan('positive-words.txt',what='character',comment.char=';')
negative=scan('negative-words.txt',what='character',comment.char=';')
# get sentiment score per city
LAresults <- score.sentiment(LA_text, positive, negative)
LAresults
head(LAresults)
str(LAresults)
summary(LAresults)
setwd("~/Desktop/Big Data and Data Science/Twitter_Sentiment")
library(RColorBrewer)
library(wordcloud)
library(tm)
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(base64enc)
library(SnowballC)
library(dplyr)
library(ggplot2)
library(qdap)
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
facebookTweetsLA <- searchTwitter('facebook', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
facebookTweetsDallas <- searchTwitter('facebook', n=2000, lang='en', geocode="32.7767,96.7970,500mi", retryOnRateLimit = 2000)
facebookTweetsNY <- searchTwitter('facebook', n=2000, lang='en', geocode="40.7128,74.0060,500mi", retryOnRateLimit = 2000)
facebookTweetsChicago <- searchTwitter('facebook', n=2000, lang='en', geocode="41.8781,87.6298,900mi", retryOnRateLimit = 2000)
facebookTweetsSF <- searchTwitter('facebook', n=2000, lang='en', geocode="37.7749,122.4194,300mi", retryOnRateLimit = 2000)
# identify & create text files to turn into a cloud
LA_text = sapply(facebookTweetsLA, function(x) x$getText())
Dallas_text = sapply(facebookTweetsDallas, function(x) x$getText())
NY_text = sapply(facebookTweetsNY, function(x) x$getText())
Chicago_text = sapply(facebookTweetsChicago, function(x) x$getText())
SF_text = sapply(facebookTweetsSF, function(x) x$getText())
# convert emojis
LA_text <- iconv(LA_text, "latin1", "ASCII", sub="")
Dallas_text <- iconv(Dallas_text, "latin1", "ASCII", sub="")
NY_text <- iconv(NY_text, "latin1", "ASCII", sub="")
Chicago_text <- iconv(Chicago_text, "latin1", "ASCII", sub="")
SF_text <- iconv(SF_text, "latin1", "ASCII", sub="")
# converting all text to lowercase
LA_text <- tolower(LA_text)
Dallas_text <- tolower(Dallas_text)
NY_text <- tolower(NY_text)
Chicago_text <- tolower(Chicago_text)
SF_text <- tolower(SF_text)
# filter out retweets
LA_text <- filter(!str_detect(LA_text, "^rt")) %>%
# remove tabs
LA_text <- gsub("[ |\t]{2,}", "", LA_text)
# filter out retweets
LA_text <- filter(!str_detect(facebookTweetsLA, "^rt")) %>%
# remove tabs
LA_text <- gsub("[ |\t]{2,}", "", LA_text)
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
facebookTweetsLA <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
# identify & create text files to turn into a cloud
LA_text = sapply(facebookTweetsLA, function(x) x$getText())
Dallas_text = sapply(facebookTweetsDallas, function(x) x$getText())
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# identify & create text files to turn into a cloud
LA_text = sapply(facebookTweetsLA, function(x) x$getText())
# convert emojis
LA_text <- iconv(LA_text, "latin1", "ASCII", sub="")
# converting all text to lowercase
LA_text <- tolower(LA_text)
# filter out retweets
LA_text <- filter(!str_detect(facebookTweetsLA, "^rt")) %>%
# remove tabs
LA_text <- gsub("[ |\t]{2,}", "", LA_text)
# remove tabs
LA_text <- gsub("[ |\t]{2,}", "", LA_text)
# remove blank spaces at the beginning
LA_text <- gsub("^ ", "", LA_text)
# remove blank spaces at the end
LA_text <- gsub(" $", "", LA_text)
# load word dictionaries
positive=scan('positive-words.txt',what='character',comment.char=';')
negative=scan('negative-words.txt',what='character',comment.char=';')
# the clean() function cleans the twitter feeds and splits the strings into a vector of words
clean=function(t){
t=gsub('[[:punct:]]','',t)
t=gsub('[[:cntrl:]]','',t)
t=gsub('\\d+','',t)
t=gsub('[[:digit:]]','',t)
t=gsub('@\\w+','',t)
t=gsub('http\\w+','',t)
t=gsub("^\\s+|\\s+$", "", t)
t=sapply(t,function(x) tryTolower(x))
t=str_split(t," ")
t=unlist(t)
return(t)
}
# define text cleaning functions
tryTolower = function(x)
{
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
tweetclean <- lapply(facebookTweetsLA, function(x) clean(x))
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
facebookTweetsLA <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
facebookTweetsDallas <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="32.7767,96.7970,500mi", retryOnRateLimit = 2000)
facebookTweetsNY <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="40.7128,74.0060,500mi", retryOnRateLimit = 2000)
library(RColorBrewer)
library(wordcloud)
library(tm)
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(base64enc)
library(SnowballC)
library(dplyr)
library(ggplot2)
library(qdap)
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
facebookTweetsLA <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
facebookTweetsDallas <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="32.7767,96.7970,500mi", retryOnRateLimit = 2000)
facebookTweetsNY <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="40.7128,74.0060,500mi", retryOnRateLimit = 2000)
facebookTweetsChicago <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="41.8781,87.6298,900mi", retryOnRateLimit = 2000)
facebookTweetsSF <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="37.7749,122.4194,300mi", retryOnRateLimit = 2000)
# identify & create text files to turn into a cloud
LA_text = sapply(facebookTweetsLA, function(x) x$getText())
Dallas_text = sapply(facebookTweetsDallas, function(x) x$getText())
NY_text = sapply(facebookTweetsNY, function(x) x$getText())
facebookTweetsSF <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="37.7749,122.4194,300mi", retryOnRateLimit = 2000)
Chicago_text = sapply(facebookTweetsChicago, function(x) x$getText())
SF_text = sapply(facebookTweetsSF, function(x) x$getText())
# convert emojis
LA_text <- iconv(LA_text, "latin1", "ASCII", sub="")
Dallas_text <- iconv(Dallas_text, "latin1", "ASCII", sub="")
NY_text <- iconv(NY_text, "latin1", "ASCII", sub="")
Chicago_text <- iconv(Chicago_text, "latin1", "ASCII", sub="")
SF_text <- iconv(SF_text, "latin1", "ASCII", sub="")
# converting all text to lowercase
LA_text <- tolower(LA_text)
Dallas_text <- tolower(Dallas_text)
NY_text <- tolower(NY_text)
Chicago_text <- tolower(Chicago_text)
SF_text <- tolower(SF_text)
# remove tabs
LA_text <- gsub("[ |\t]{2,}", "", LA_text)
Dallas_text <- gsub("[ |\t]{2,}", "", Dallas_text)
NY_text <- gsub("[ |\t]{2,}", "", NY_text)
Chicago_text <- gsub("[ |\t]{2,}", "", Chicago_text)
SF_text <- gsub("[ |\t]{2,}", "", SF_text)
# remove blank spaces at the beginning
LA_text <- gsub("^ ", "", LA_text)
Dallas_text <- gsub("^ ", "", Dallas_text)
NY_text <- gsub("^ ", "", NY_text)
Chicago_text <- gsub("^ ", "", Chicago_text)
SF_text <- gsub("^ ", "", SF_text)
# remove blank spaces at the end
LA_text <- gsub(" $", "", LA_text)
Dallas_text <- gsub(" $", "", Dallas_text)
NY_text <- gsub(" $", "", NY_text)
Chicago_text <- gsub(" $", "", Chicago_text)
SF_text <- gsub(" $", "", SF_text)
# concatenate files together
facebookTweets <- c(LA_text, Dallas_text, NY_text, Chicago_text, SF_text)
# create corpus
facebookTweets.corpus <- Corpus(VectorSource(facebookTweets))
# remove all punctuation
facebookTweets.corpus <- tm_map(facebookTweets.corpus, removePunctuation)
# remove stopwords
facebookTweets.corpus <- tm_map(facebookTweets.corpus, function(x)removeWords(x, stopwords()))
# remove URLs from text
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
facebookTweets.corpus <- tm_map(facebookTweets.corpus, content_transformer(removeURL))
# build a term-document matrix
facebookTweets_matrix <- TermDocumentMatrix(facebookTweets.corpus)
facebookTweets_matrix <- as.matrix(facebookTweets_matrix)
facebookTweets_matrix <- sort(rowSums(facebookTweets_matrix), decreasing = TRUE)
# converting words to dataframe
facebookTweets_matrix <- data.frame(word = names(facebookTweets_matrix), freq = facebookTweets_matrix)
head(facebookTweets_matrix, 10)
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
zuckerbergTweetsLA <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
zuckerbergTweetsDallas <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="32.7767,96.7970,500mi", retryOnRateLimit = 2000)
zuckerbergTweetsNY <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="40.7128,74.0060,500mi", retryOnRateLimit = 2000)
zuckerbergTweetsChicago <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="41.8781,87.6298,900mi", retryOnRateLimit = 2000)
zuckerbergTweetsSF <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="37.7749,122.4194,300mi", retryOnRateLimit = 2000)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
zuckerbergTweetsLA <- searchTwitter('zuckerberg -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,1000mi", retryOnRateLimit = 2000)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
zuckerbergTweetsLA <- searchTwitter('zucc -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,1000mi", retryOnRateLimit = 2000)
# download the curl certification
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
# define OAuth URLS
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
# set up authorization
consumerKey <- "NYk2xN996KGM32qXk4k2aaRLp"
consumerSecret <- "Jt7574haxzOX7XMAStDV0xUrWZRWblbe8afHjvQlyYLN7PrVY6"
accessToken <- "384695289-ex2DOu04QTyJgpzusO4Pua9EV9p4E9pw60fhWUVJ"
accessTokenSecret <- "6etmu5lqRnuYIWYMDgJnChksA0YCNIrLJZwvmNJdSzZb1"
setup_twitter_oauth(consumerKey,
consumerSecret,
accessToken,
accessTokenSecret)
# extract the tweets
# NOTE: I had to increase the radius because not enough tweets were being returned
facebookTweetsLA <- searchTwitter('facebook -filter:retweets', n=2000, lang='en', geocode="34.0522,118.2437,900mi", retryOnRateLimit = 2000)
setwd("~/Desktop/Dev. Business Apps using SQL/project/sentimentAnalysis/original_files")
library(RColorBrewer)
library(wordcloud)
library(tm)
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(base64enc)
library(SnowballC)
library(dplyr)
library(ggplot2)
library(qdap)
library(RCurl)
library(googleLanguageR)
library(googleAuthR)
library(googleLanguageR)
library(googleAuthR)
# read in csv files
democrats = read.csv("subreddit_Democrats.csv", header = TRUE, stringsAsFactors = FALSE)
republicans = read.csv("subreddit_Republicans.csv", header = TRUE, stringsAsFactors = FALSE)
news = read.csv("subreddit_News.csv", header = TRUE, stringsAsFactors = FALSE)
politics = read.csv("subreddit_Politics.csv", header = TRUE, stringsAsFactors = FALSE)
trueReddit = read.csv("subreddit_TrueReddit.csv", header = TRUE, stringsAsFactors = FALSE)
usNews = read.csv("subreddit_USNews.csv", header = TRUE, stringsAsFactors = FALSE)
politicalDiscussion = read.csv("subreddit_PoliticalDiscussion.csv", header = TRUE, stringsAsFactors = FALSE)
# CLEANING THE TEXT
# extract the text
democrats <- democrats$Title
republicans <- republicans$Title
news <- news$Title
politics <- politics$Title
trueReddit <- trueReddit$Title
usNews <- usNews$Title
politicalDiscussion <- politicalDiscussion$Title
# convert emojis
democrats <- iconv(democrats, "latin1", "ASCII", sub="")
republicans <- iconv(republicans, "latin1", "ASCII", sub="")
news <- iconv(news, "latin1", "ASCII", sub="")
politics <- iconv(politics, "latin1", "ASCII", sub="")
trueReddit <- iconv(trueReddit, "latin1", "ASCII", sub="")
usNews <- iconv(usNews, "latin1", "ASCII", sub="")
politicalDiscussion <- iconv(politicalDiscussion, "latin1", "ASCII", sub="")
# converting all text to lowercase
democrats <- tolower(democrats)
republicans <- tolower(republicans)
news <- tolower(news)
politics <- tolower(politics)
trueReddit <- tolower(trueReddit)
usNews <- tolower(usNews)
politicalDiscussion <- tolower(politicalDiscussion)
# authenticate with Google language API services
gl_auth("googleRedditAPI.json")
# NLP SENTIMENT ANALYSIS
nlpDemocrats <- gl_nlp(democrats, Sys.sleep(3))
nlpRepublicans <- gl_nlp(republicans, Sys.sleep(3))
nlpNews <- gl_nlp(news, Sys.sleep(3))
# authenticate with Google language API services
gl_auth("googleRedditAPI.json")
# NLP SENTIMENT ANALYSIS
nlpDemocrats <- gl_nlp(democrats, Sys.sleep(3))
nlpRepublicans <- gl_nlp(republicans, Sys.sleep(3))
nlpNews <- gl_nlp(news, Sys.sleep(3))
nlpPolitics <- gl_nlp(politics, Sys.sleep(3))
nlpTrueReddit <- gl_nlp(trueReddit, Sys.sleep(3))
nlpUsNews <- gl_nlp(usNews, Sys.sleep(3))
nlpPoliticalDiscussion <- gl_nlp(politicalDiscussion, Sys.sleep(3))
# extract the document sentiment
nlpDemocrats_DocSent <- nlpDemocrats$documentSentiment
nlpRepublicans_DocSent <- nlpRepublicans$documentSentiment
nlpNews_DocSent <- nlpNews$documentSentiment
nlpPolitics_DocSent <- nlpPolitics$documentSentiment
nlpTrueReddit_DocSent <- nlpTrueReddit$documentSentiment
nlpUsNews_DocSent <- nlpUsNews$documentSentiment
nlpPoliticalDiscussion_DocSent <- nlpPoliticalDiscussion$documentSentiment
# convert text files to dataframes to prepare for merging
democrats_df <- as.data.frame(democrats)
republicans_df <- as.data.frame(republicans)
news_df <- as.data.frame(news)
politics_df <- as.data.frame(politics)
trueReddit_df <- as.data.frame(trueReddit)
usNews_df <- as.data.frame(usNews)
politicalDiscussion_df <- as.data.frame(politicalDiscussion)
# make ID columns in the document sentiment and dataframes
democrats_df$ID <- seq.int(nrow(democrats_df))
republicans_df$ID <- seq.int(nrow(republicans_df))
news_df$ID <- seq.int(nrow(news_df))
politics_df$ID <- seq.int(nrow(politics_df))
trueReddit_df$ID <- seq.int(nrow(trueReddit_df))
usNews_df$ID <- seq.int(nrow(usNews_df))
politicalDiscussion_df$ID <- seq.int(nrow(politicalDiscussion_df))
nlpDemocrats_DocSent$ID <- seq.int(nrow(nlpDemocrats_DocSent))
nlpRepublicans_DocSent$ID <- seq.int(nrow(nlpRepublicans_DocSent))
nlpNews_DocSent$ID <- seq.int(nrow(nlpNews_DocSent))
nlpPolitics_DocSent$ID <- seq.int(nrow(nlpPolitics_DocSent))
nlpTrueReddit_DocSent$ID <- seq.int(nrow(nlpTrueReddit_DocSent))
nlpUsNews_DocSent$ID <- seq.int(nrow(nlpUsNews_DocSent))
nlpPoliticalDiscussion_DocSent$ID <- seq.int(nrow(nlpPoliticalDiscussion_DocSent))
head(democrats_df)
head(nlpDemocrats_DocSent)
# merge document sentiment and dataframe for each subreddit
democratScores <- merge(democrats_df, nlpDemocrats_DocSent, by = 'ID')
republicanScores <- merge(republicans_df, nlpRepublicans_DocSent, by = 'ID')
newsScores <- merge(news_df, nlpNews_DocSent, by = 'ID')
politicsScores <- merge(politics_df, nlpPolitics_DocSent, by = 'ID')
trueRedditScores <- merge(trueReddit_df, nlpTrueReddit_DocSent, by = 'ID')
usNewsScores <- merge(usNews_df, nlpUsNews_DocSent, by = 'ID')
politicalDiscussionScores <- merge(politicalDiscussion_df, nlpPoliticalDiscussion_DocSent, by = 'ID')
# write tweets with sentiment results to csv
write.csv(democratScores, file = 'democratScores.csv')
write.csv(republicanScores, file = 'republicanScores.csv')
write.csv(newsScores, file = 'newsScores.csv')
write.csv(politicsScores, file = 'politicsScores.csv')
#### ENTITY ANALYSIS
democratEntityAnalysis <- gl_nlp(democrats, nlp_type = c("analyzeEntitySentiment"), Sys.sleep(3))
str(democratEntityAnalysis)
democratEntityAnalysis <- gl_nlp(democrats, nlp_type = c("analyzeEntities"), Sys.sleep(3))
democratEntities <- democratEntityAnalysis$entities
summary(democratEntities)
democratEntities[2]
democratEntities[[2]]
View(democratEntities)
View(democratEntityAnalysis)
democratEntities[[0]]
democratEntities[[1]]
democratEntities[[,1]]
democratEntities[[1]]
democratEntities[1]
summary(democratEntities)
str(democratEntities)
democratEntities$
democratEntities$rypw
col(democratEntities)
democratEntities <- unlist(democratEntities)
str(democratEntities)
democratEntityAnalysis <- gl_nlp(democrats, nlp_type = c("analyzeEntities"), Sys.sleep(3))
democratEntities <- unlist(democratEntities)
str(democratEntities)
democratEntityAnalysis <- gl_nlp(democrats, nlp_type = c("analyzeEntities"), Sys.sleep(3))
democratEntities <- democratEntityAnalysis$entities
str(democratEntities)
str(democratEntities)
summary(democratEntities)
democratEntities$9
head(democratEntities)
democratEntities$[[,2]]
democratEntities$[[1]][2]
democratEntities[[1]][2]
democratEntities[[]][2]
democratEntities[[,]][2]
democratEntities[[1]][2]
democratEntities[[#]][2]
democratEntities[[4]][2]
democratEntities[[4]][2]
democratEntities[[4]][2]
democratEntities[[*]][2]
democratEntities[[4]][2]
democratEntities[[4]][2] == 'PERSON'
democratEntities[2] == 'PERSON'
democratEntities[[4]][2] == 'PERSON'
entities[democratEntityAnalysis]
democratEntities[[_]][2] == 'PERSON'
democratEntities[[]][2] == 'PERSON'
democratEntities[[]][2] == 'PERSON'
democratEntities[[,]][2] == 'PERSON'
democratEntities[[$]][2] == 'PERSON'
democratEntities[[]][2] == 'PERSON'
entities[democratEntityAnalysis]
democratEntities[[]][2] == 'PERSON'
democratEntities[[1]][2] == 'PERSON'
democratEntities[[1:50]][2] == 'PERSON'
democratEntities[1:50][2] == 'PERSON'
democratEntities[1:50][[2] == 'PERSON'
democratEntities[1:50][[2]] == 'PERSON'
democratEntities[1:50][2] == 'PERSON'
democratEntities[,1:50][2] == 'PERSON'
democratEntities[1:50 ,][2] == 'PERSON'
democratEntities[1:50][2] == 'PERSON'
democratEntities[1:50][2]
== 'PERSON'
democratEntities[1:50][2] == 'PERSON'
View(democratEntities)
democratEntities[[1:50]][2] == 'PERSON'
democratEntities[1:50][2] == 'PERSON'
democratEntities[1:50][2] == 'PERSON'
democratEntities[1:50][2] = 'PERSON'
View(democratEntities)
democratEntities[1:50]][2] == 'PERSON'
str(democratEntities)
democratEntities[1:50][2] == 'PERSON'
democratEntities[1:50][2]
democratEntityAnalysis <- gl_nlp(democrats, nlp_type = c("analyzeEntities"), Sys.sleep(3))
democratEntities <- democratEntityAnalysis$entities
str(democratEntities)
head(democratEntities)
democratEntities[1:50][2]
democratEntities[1:50][2]
democratEntities[1:50][[2]]
democratEntities[1:50][2]
